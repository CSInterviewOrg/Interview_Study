## 가상화란 무엇이며, 왜 운영 체제에서 중요한가요?
### 가상화
- 컴퓨터가 하드웨어 리소스를 디지털로 분리된 여러 환경과 공유할 수 있도록 하는 프로세스
- 가상화를 사용하면 하드웨어 리소스와 상호 작용할 때 유연성이 좋음
- 전기를 소비하고 스토리지 공간을 차지하며 유지 관리를 필요로 하는 물리적 서버 기능을 소프트웨어로 추상화함으로써 제한을 제거
- 여러 개의 물리적 서버를 설정하지 않고 단일 물리적 서버에 여러 개의 가상 머신을 생성 → 가상 머신에 대한 운영체제 요구 사항을 지정하고 물리적 서버와 같은 방식으로 사용 가능 (하드웨어 경비는 줄어듬)

## 운영 체제가 CPU, 메모리, 디스크를 가상화한 것은 무엇인가요?
- CPU 가상화: 프로세스가 CPU를 사용할 때 운영체제가 시간 분할 방식으로 CPU 시간을 할당하여 프로세스가 동시 실행되는 것처럼 보이게 함(스케줄링 알고리즘 사용)
- 메모리 가상화: 각 프로세스는 가상 주소 공간을 가지며 운영체제는 물리 메모리와 가상 메모리 간의 매핑 관리
- 디스크 가상화: 운영체제는 디스크 공간을 논리적으로 나누어 각 프로세스가 독립적인 파일 시스템을 사용하는 것처럼 보이게 함

## 운영체제에서 커널이란 무엇이며 시스템 콜에 대해서 설명해주세요
### 커널
- 항상 메모리에 상주하며 컴퓨터 자원을 관리하는 자원 관리자의 역할 - 사용자가 물리적 하드웨어에 접근하고 사용할 수 있도록 해줌
- 역할 4가지
  - 메모리 관리: 각 프로그램의 사용량 추적, 메모리 자원 할당
  - 프로세스 관리 및 CPU 스케줄링: CPU 시간 자원 배분
  - 디바이스 관리: 컴퓨터에 연결된 장치를 드라이버를 통해 관리
  - 시스템 콜 인터페이스 및 보안
### 시스템 콜
- 사용자나 응용 프로그램이 컴퓨터 자원을 사용하기 위해 호출 - 운영체제는 시스템 콜을 통해서만 커널에 접근할 수 있음(컴퓨터 자원 보호)
- 시스템 콜을 사용할 때, 프로그램은 특정함수나 라이브러리 호출을 통해 커널과 상호작용하고 이 과정에서 사용자 모드에서 커널모드로 전환 ⇒ 안정성, 보안성 확보

- 사용자 모드
  - 응용 프로그램이 실행되는 영역
  - 사용자가 접근할 수 있는 영역을 제한적으로 두고 프로그램의 자원에 접근하지 못하도록 설정하는 모드
- 커널 모드
  - 자원에 대한 제어권을 가지고 모든 자원에 접근하여 가능한 모든 명령어 실행
  - 커널모드는 바로 접근할 수 없고 커널모드 내부 함수 시스템 콜을 호출해야 함

## 인터럽트란 무엇이며 인터럽트와 시스템 콜의 차이점에 대해서 설명해주세요.
### 인터럽트
- 하드웨어나 소프트웨어가 CPU의 실행 흐름을 중단하고 특정 작업을 처리하도록 커널에 신호를 보내는 메커니즘
- 종류
  - 하드웨어 인터럽트: 외부장치에서 발생하며 이벤트가 발생했음을 CPU에 알림
  - 소프트웨어 인터럽트: 프로그램 실행 중 발생하여 특정조건이 충족될 때 발생
### 시스템 콜과의 차이점
- 발생 원인
  - 인터럽트: 하드웨어, 소프트웨어 이벤트로 발생
  - 시스템콜: 프로그램이 커널의 특정 기능을 요청할 때 발생
- 목적
  - 인터럽트: CPU가 하드웨어의 요청이나 상태 변화에 즉각적으로 반응
  - 시스템콜: 사용자 프로그램이 커널의 서비스를 이용할 수 있도록 함
- 처리 방식
  - 인터럽트: 커널은 인터럽트 발생시 현재 실행중인 프로세스를 중단하고 인터럽트 처리 후 다시 원래 프로세스 실행
  - 시스템콜: 프로그램이 시스템콜 실행하면 CPU는 사용자 모드에서 커널모드로 전환하여 요청 작업 수행
 
## 멀티프로세싱 시스템에서 스케줄링이 중요한 이유는 무엇인가?
1. 리소스 효율성: 멀티 프로세싱 환경에는 여러 CPU가 존제하므로 각 프로세스가 적절한 CPU에서 실행될 수 있도록 효율적인 리소스 분배 → CPU 활용도 극대화
2. 공정성: 모든 프로세스에게 공정하게 CPU 시간 할당 → 특정 프로세스가 오랜 시간 대기하지 않도록 하여 시스템 안정성 유지
3. 우선순위 관리: 중요한 작업이 우선적으로 신속하게 처리되도록 하여 사용자 경험 향상
4. 작업간 상호작용 관리: 여러 프로세스가 동시에 실행되기 때문에 이들 간 상호작용, 자원 공유 관리
5. 로드 밸런싱: 프로세스들이 여러 CPU에 고르게 분산되도록하여 과부하 방지, 시스템 성능 최적화

## 컨텍스트 스위칭이란 무엇이며, 컨텍스트 스위칭에 따라 오버헤드가 발생하는 이유가 무엇인가요?
### 컨텍스트 스위칭
- 여러개의 프로세스가 실행되고 있을 때 기존에 실행되던 프로세스를 중단하고 다른 프로세스를 실행하는 것 (cpu가 실행할 프로세스를 교체하는 기술)
- 동작 순서
  - A 실행중, B 대기중
  - 스케줄러가 A의 실행을 중단하고 B 실행 요청
  - A는 스택의 데이터 위치를 가리키는 포인터의 값과 다음 실행해야 하는 주소값을 가지고 있는 프로그램 카운터의 값을 PCB에 저장
  - A는 대기중 또는 block 상태로 바뀌고 b 실행(b가 실행 중으로 변경)
- 단점
  - 과도하고 발생할 경우 오버헤드 발생
### 오버헤드 발생 원인
- 상태 정보를 저장하고 복원하는 과정에서 메모리에 접근해야 하고 이때 많은 시간이 소요됨
- 각 프로세스에 대한 정보를 관리하기 위해 프로세스 테이블을 유지해야 함 ⇒ 상태를 계속 업데이트 해야 하기 때문에 추가적인 작업 필요
- 스케줄링 알고리즘에 따라 어떤 프로세스를 실행할지 결정해야 함 ⇒ 결정 과정도 추가적인 연산 필요
- 컨텍스트 스위칭이 빈번하게 발생하면 프로세스 간 전환에 소요되는 시간과 자원이 시스템 성능에 영향을 줄 수 있음

## CPU 스케줄링 알고리즘의 종류와 특징은?(라운드 로빈, 우선순위 스케줄링, 멀티 레벨 큐)
- FCFS(First Come First Served)
  - 비선점 스케줄링: 이미 할당된 cpu를 다른 프로세스가 강제로 뺏어 사용할 수 없는 스케줄링 기법
  - 프로세스 응답 시간 예측 용이, 일괄 처리 방식(여러개의 프로그램을 읽고 한번에 하나의 프로그램만 실행)에 적합
  - 먼저 도착한 프로세스가 먼저 cpu 선점
  - 단점 - Convoy Effect: 실행 시간이 짧은 프로세스들이 실행시간이 긴 프로세스를 계속해서 기다리면서 효율성 저하
- SJF(Shortest Jop First)
  - 실행시간이 가장 짧은 프로세스부터 실행
  - 비선점 스케줄링이기 때문에 이미 실행된 프로세스가 있다면 새로 도착한 프로세스는 더 짧아도 대기 필요
  - 단점 - Starvation: 실행시간이 긴 프로세스가 영원히 CPU를 할당받지 못함
- SRTF(Shortest Remaining Time First)
  - 선점형 스케줄링: 현재 실행 중인 프로세스보다 우선순위가 높은 프로세스가 도착하면 cpu 뺏김
  - 새로운 프로세스가 도착할 때마다 새롭게 스케줄링 진행
  - 새로운 프로세스가 들어온 시점에서 가장 실행시간이 짧은 프로세스 먼저 실행
  - 단점 - Starvation: SJF와 동일
  - 단점 - 새로운 프로세스가 올 때마다 스케줄링을 다시 실행하기 때문에 정확한 CPU burst time 측정 불가
- Priority Scheduling
  - 우선순위가 높은 프로세스가 CPU를 선점하도록 하는 스케줄링
  - 선점, 비선점 스케줄링 방식 모두 사용 가능
  - 단점 - 우선순위가 낮을 수록 계속 밀려 실행 불가
  - 단점 - 무기한 봉쇄: 우선순위가 높은 프로세스가 blocking 되어 있어 cpu가 계속해서 대기하는 상황
  - 해결방법 - 대기 시간이 증가할수록 우선순위를 높여주어 예방 가능
- Round-Robin
  - 각 프로세스는 동일한 할당 시간을 가짐
  - cpu를 할당 받고 할당 시간이 지나면 레디 상태로 돌아가 큐에 들어감
  - 프로세스들이 작업을 완료할 때까지 계속해서 순회
  - 장점 - Response Time이 빨라짐
  - 장점 - 모든 프로세스가 공정하게 cpu 할당 보장
- Multi Level Queue
  - 프로세스를 여러 개의 큐로 나누어 각각의 큐에 따라 서로 다른 스케줄링 알고리즘 적용하는 방법
  - 프로세스는 우선순위, 성격, 요구 자원 등의 기준에 따라 분류
  - 장점 - 서로 다른 특성을 가진 프로세스를 효율적으로 처리 가능, 중요도에 따라 우선순위 정해서 처리 가능
  - 단점 - 프로세스가 특정 큐에 고정되면 우선순위 낮은 프로세스가 오랜 시간 대기해야함, 구현 복잡

## 프로세스와 스레드의 차이점은 무엇인가?
### 프로세스
- 실행 중인 프로그램의 인스턴스
- 독립적인 메모리 공간을 가지며 다른 프로세스와 메모리를 공유하지 않음
- 프로세스 간 컨텍스트 스위칭이 복잡하고 많은 시간 소요됨
- 한 프로세스에서 발생한 오류는 다른 프로세스에 영향을 주지 않음
### 쓰레드
- 프로세스 내에서 실행되는 실행 단위
- 같은 프로세스 내의 여러 스레드는 메모리 공간을 공유함, 쓰레드 간 통신이 더 빠름
- 비교적 컨텍스트 스위칭이 빠름
- 한 스레드 내에서 발생한 오류가 다른 스레드에 영향을 줌

## PCB(Process Control Block)란 무엇이며, 어떤 정보를 포함하나요?
### PCB(Process Control Block)
- 운영체제가 각 프로세스를 관리하기 위해 사용되는 데이터 구조
- 프로세스의 상태와 관련된 정보를 저장하고 프로세스 생성, 실행, 대기, 종료 과정 추적할 때 사용 ⇒ 기반으로 컨텍스트 스위칭 수행(원활하게 여러 프로세스가 동시에 실행되게 해줌)
- 포함하는 정보
  - 프로세스 식별자(PID)
  - 프로세스 상태
  - 프로세스 우선순위(스케줄링에 사용)
  - 프로세스 카운터(PC): 다음에 실행할 명령어 주소 저장
  - CPU 레지스터: 프로세스가 실행 중일 때의 CPU 레지스터 값 포함 → 프로세스 재실행 시 필요
  - 메모리 관리 정보
  - I/O 상태 정보
  - 프로세스의 통계 정보
  - 연결 정보

## 프로세스 주소 공간이란 무엇이며, 프로세스마다 고유한 주소 공간을 가지는 이유는?
### 프로세스 주소 공간
- 각 프로세스가 사용할 수 있는 메모리 주소 범위
- 주소 공간 구성
  - 텍스트 영역: 실행할 프로그램 코드 저장
  - 데이터 영역: 전역 변수와 정적 변수 저장
  - 힙 영역: 동적으로 할당된 메모리 저장
  - 스택 영역: 함수 호출시 지역 변수, 함수의 매개변수, 반환 주소 저장
- 프로세스마다 고유한 주소 공간 가지는 이유
  - 각 프로세스가 독립적인 주소 공간을 갖기 때문에 다른 프로세스가 메모리에 접근 불가 → 프로세스 간 간섭 방지, 시스템 안정성 높임, 데이터 안전하게 보호
  - 프로세스 종료시 해당 주소 공간은 해제되어 다른 프로세스가 사용 가능 → 자원 관리
  - 컨텍스트 스위칭이 간편해짐 → 각 프로세스의 상태를 PCB에 저장하고 복원할 수 있어 프로세스를 효율적으로 스케줄링 가능

## 프로세스 상태 전이(Process State Transition)를 설명해보세요.
### 프로세스
실행 중에 있는 프로그램 = Job, Task
### 상태
- New(생성 상태): 프로세스가 생성되고 있는 상태, 프로세스가 시스템에 등록되고 필요한 자원을 할당 받기 전 단계
- Ready(준비 상태): 프로세스가 실행될 준비가 완료된 상태, CPU 할당 받기 전 단계
- Running(실행 상태): CPU에 할당되어 실제로 명령어 실행중인 상태
- Waiting(대기 상태): I/O 작업이나 다른 이벤트가 완료되기를 기다리는 상태(CPU를 사용하지 않음)
- Terminated(종료 상태): 프로세스의 실행이 완료되거나 종료된 상태, 더이상 프로세스가 실행되지 않으며 자원 해제
### 프로상태 전이
운영체제에서 프로세스의 상태가 변화하는 과정
- Dispatch(Ready → Running): 스케줄러에 의해 CPU가 할당되어 실행상태로 전이
- Interrupt(Running → Ready): 할당된 CPU 시간이 지나면 Timeout Interrupt가 발생하여 CPU를 다른 프로세스에 양도하고 기존 프로세스는 준비상태로 전이
- Block(Running → Waiting): 자원 요청 후 즉시 할당 받지 못해 할당 받을 때까지 기다리는 과정
- Wakeup(Waiting → Ready): 필요한 자원이 할당되어 프로세스가 준비 상태로 전이

## 프로세스 간 통신(IPC: Interprocess Communication) 방법에는 무엇이 있나요? (파이프, 메시지 큐, 공유 메모리 등)
### 프로세스 간 통신
- 프로세스들이 데이터를 공유하고 작업을 조율하기 위해 사용하는 다양한 메커니즘
- 효율적인 시스템 운영과 자원관리 가능
### 방법
- 파이프(Pipes)
  - 단방향 통신을 위한 기본적인 IPC 메커니즘
  - 한 프로세스의 출력이 다른 프로세스의 입력으로 직접 연결
  - 부모-자식 프로세스 간에 사용되고 데이터 스트림 전송 시 사용
- 이름있는 파이프(Named Pipe)
  - 양방향 통신 지원
  - 시스템 내의 어떤 프로세스 간에도 사용 가능
  - 파일 시스템 내에 특정 이름을 가진 파일 형태로 존재하고 이를 통해 프로세스가 데이터 교환 가능
- 메시지 큐(Messages Queues)
  - 프로세스 간 메시지를 전송하기 위한 메커니즘
  - 메시지는 FIFO 순서로 전송되고 프로세스는 비동기적으로 메시지 송수신 가능
  - 복잡한 데이터 구조 전송, 메시지의 우선순위를 설정할 수 있는 기능 제공
- 공유 메모리(Shared Memory)
  - 두개 이상의 프로세스가 시스템 메모리의 동일한 부분을 공유하여 데이터 접근, 변경 가능
  - 매우 빠른 데이터 접근, 전송이 가능하지만 동기화 메커니즘을 별도로 사용해야함
- 세마포어(Semaphores)
  - 주로 동기화 목적으로 사용, 간접적으로 프로세스 간 통신에도 사용
  - 공유 자원에 대한 접근 제어, 데이터 일광성, 동시성 유지
- 소켓(Sockets)
  - 네트워크를 통한 프로세스 간 통신을 가능하게 하는 메커니즘
  - TCP/IP, UDP 프로토콜 사용
  - 같은 시스템 내의 프로세스, 다른 시스템에 있는 프로세스 간 데이터 교환 가능
- 시그널(Signals)
  - 프로세스에 특정 이벤트가 발생했음을 알리는 간단 메시지
  - 운영체제 또는 프로세스로부터 프로세스에 비동적으로 알림을 전달할 때 사용

## 멀티스레드(Multithreading)의 장점과 단점은 무엇인가요?
### 멀티 쓰레드
- 하나의 프로세스 안에서 여러 개의 쓰레드를 사용하는 것
- 다양한 작업을 동시 수행 가능
- 장점
  - 자원 효율성
    - 동일한 프로세스 내에서 실행되므로 메모리와 자원 공유 가능 → IPC 를 사용하지 않고도 데이터 공유 가능
  - 응답성
    - 한 프로세스 내에서 여러 동작을 수행할 수 있기 때문에 오버헤드가 감소해서 응답시간이 빠름
  - 컨텍스트 스위칭 비용 감소
    - 프로세스보다 비교적 오버헤드가 적음
    - 스레드 간 공유 자원을 제외하고 스레드 정보만 교체하면 되기 때문에 비교적 비용이 낮음
- 단점
  - 동기화
    - 여러 스레드가 공유 자원에 동시 접근이 가능하기 때문에 동기화 문제 발생 가능 → 여러 스레드가 한 자원을 변경하면 의도하지 않은 값을 출력하여 버그 발생
    - 동기화 작업을 수행하면 여러 스레드 접근을 제한하기 때문에 병목 현상 발생 가능 → 임계 영역에 대한 뮤텍스, 세마포어 방식 활용
  - 안정성
    - 하나의 스레드에 문제가 생기면 다른 스레드에도 영향을 줌
  - 오버헤드
    - 스레드 수가 많아질수록 컨텍스트 스위칭이 많이 발생하고 성능 저하 발생 가능
  - 디버깅
    - 여러 스레드가 동시에 수행되기 때문에 각 스레드의 동작을 추적하기 어려움

## 스레드 동기화(Thread Synchronization)가 중요한 이유는 무엇인가요?
- 스레드 간의 상호작용 조정, 효율적인 작업 처리 보장을 위해 중요
- 순차적 작업 수행 보장: 특정 순서로 작업이 실행되어야 하는 경우 동기화를 통해 흐름 제어
- 작업 간 의존성 관리: 스레드가 서로 의존하는 작업을 수행하는 경우 동기화를 통해 의존성 명확히 설정
- 이벤트 기반 처리: 스레드가 특정 이벤트를 기다리는 경우, 동기화를 통해 스레드 흐름 제어
- 성능 최적화: 스레드가 필요할 때만 실행되도록 제어할 수 있어 시스템 자원 낭비 축소, 불필요한 컨텍스트 스위칭 줄임

## 동시성 문제란 무엇인가?
- 여러 스레드나 프로세스가 동시에 공유 데이터에 접근할 때 발생할 수 있는 오류와 비정상적인 동작
- 동시성 문제로 데이터 정합성이 깨지는 경우
  - Race Condition(경쟁상태): 여러 스레드가 동시에 데이터를 읽고 수정할 때 발생
  - 데이터 불일치: 한 스레드가 데이터를 변경하는 동안 다른 스레드가 이전 데이터를 읽어 불일치 발생
  - 비가용성: 데이터가 일관된 상태를 유지하지 못하는 상황

## 교착 상태(Deadlock)란 무엇인가요? 교착 상태의 해결 방법을 설명해보세요.
### 데드락(교착상태)
두개 이상의 프로세스가 서로의 작업이 끝날 때까지 무한정 대기하는 현상
### 해결방법
- 회피: 교착상태가 발생하지 않도록 자원 요청 관리, 자원 할당 시 각 프로세스의 최대 요구량을 미리 알고 이를 기반으로 안전상태 유지(동적으로 제어)
- 탐지 및 회복: 주기적으로 교착 상태 탐지 → 프로세스를 종료하거나 자원을 강제로 회수하여 회복
- 예방: 발생하지 않도록 시스템 설계 단계에서 모든 조건을 제거, 네가지 교착 상태 발생 조건 중 하나를 제거하여 원천적으로 차단(정적으로 제어)
- 임의적 프로세스 종료: 교착상태 발생시 프로세스의 우선순위, 자원 소비량, 실행 시간 등을 고려하여 특정 프로세스 종료

## 임계 구역(Critical Section)이란 무엇이며, 이를 해결하는 방법은 무엇인가요?
### 임계구역
- 여러 스레드가 동시에 접근할 수 없는 공유 자원에 대한 코드의 일부분
- 데이터의 일관성을 보장하기 위해 한번에 하나의 프로세스만 접근해야 함 → 여러 개의 스레드가 동시에 임계구역에 들어가면 데이터 손상 등이 발생 가능
### 요구 조건
- 상호 배제: 한 프로세스가 임계구역에 들어가면 다른 프로세스는 해당 임계구역에 들어갈 수 없음
- 진행: 임계구역에 들어가려는 프로세스가 대기 중일 때 임계구역에 있는 프로세스가 완료되면 대기 중인 프로세스 중 하나가 임계구역에 들어갈 수 있음
- 한정 대기: 임계구역에 들어가려는 프로세스가 대기하는 시간을 제한 → 무한 대기하는 상황 제어
### 해결방법
- 뮤텍스(Mutex): 상호배제를 보장하는 메커니즘, 하나의 스레드가 임계구역에 들어가면 뮤텍스를 잠그고 다른 스레드는 해당 뮤텍스가 해제될 때까지 대기
- 세마포어(Semaphore): 카운터를 사용하는 동기화 기법, 특정 자원의 사용 가능 수 관리
- 모니터(Monitors): 공유 자원에 대한 접근을 조정하며 내장된 뮤텍스와 조건 변수를 사용하여 상호 배제 제공
- 알고리즘
  - Decker’s Algorithm: 두 프로세스가 번갈아 가며 임계구역에 진입하는 방식
  - Peterson’s Algorithm: 프로세스간 우선순위를 조정하여 상호배제 구현
  - Bakery Algorithm: 번호표를 뽑는 방식으로 임계구역 접근 순서 결정

## 상호 배제(Mutual Exclusion)란 무엇인가요?
- 여러 스레드가 공유자원에 접근할 때 동시에 접근하지 못하도록 보장
- 임계구역 내에 있는 자원에 대해서 하나의 프로세스만 접근할 수 있도록 보장
- 데이터 무결성과 일관성, 시스템 안정성 보장

## 뮤텍스(Mutex)와 세마포어(Semaphore)의 차이점은 무엇인가요?
### 뮤텍스
Locking 메커니즘으로 하나의 스레드만 동일 시점에 뮤텍스를 얻어 임계영역에 접근 가능 → 임계영역을 나갈 때 뮤텍스 해제
### 세마포어
- Signaling 메커니즘으로 락을 걸지 않은 스레드도 시그널을 보내 락 해제 가능
- wait을 호출하면 세마포어의 카운트를 1 줄이고 카운트가 0보다 작거나 같아지면 락 실행
- 다른 스레드가 signal 함수를 호출하면 카운트가 1 증가하고 해당 스레드는 락에서 나올 수 있음
### 차이점
- 뮤텍스는 임계구역을 보호하기 위해 사용되고 세마포어는 사용 가능한 커넥션 수를 제한할 때 사용
- 뮤텍스는 소유자가 있고 세마포어는 소유자가 없음
- 뮤텍스는 잠금, 해제 두개의 상태만 가지고 세마포어는 카운트 하기 때문에 다수의 자원 관리
- 뮤텍스는 상호배제를 제공하는 단일 스레드 동기화 메커니즘 ↔ 세마포어는 자원의 수를 관리하는 카운트 메커니즘

## 스핀락(Spinlock)과 같은 동기화 기법은 언제 사용되나요?
### 스핀락
- 스레드가 잠금이 해제될 때까지 반복해서 검사하면서 대기
- 장점: 간단하고 빠른 기법으로 오버헤드가 적고 빠른 실행 경로가 필요할 경우 사용
- 단점: 스레드가 계속해서 CPU를 사용하기 때문에 자원 낭비 가능, 교착상태에 빠질 수 있음, 대기 시간이 길어질 경우 비효율적 → 자원 소모
- 사용하는 상황
  - 임계구역의 실행 시간이 짧을 경우
  - 멀티 프로세서 시스템: 잠금을 해제 할 때까지 대기하므로 프로세서 간 전환 비용 최소화
  - 응답 시간이 중요한 실시간 시스템

## Readers-Writers, 생산자-소비자 문제와 식사하는 철학자 문제에 대해서 설명해주세요.
경쟁상태와 교착상태와 관련된 고전 문제
### Readers-Writers 문제
- 여러 Reader가 데이터에 접근하는 것은 괜찮지만, Writer는 독점적으로 사용해야 함
- 교착 상태나 기아 상태를 방지 하면서 접근 보장
- 해결 방법: 우선순위 방식
    - Reader 우선방식: Reader가 먼저 접근하고 다 끝난 후에 Writer 접근
    - Writer 우선 방식: Writer 접근 우선, Writer 대기 중이면 Reader 접근 제안
- 읽기 위주의 시스템에서 사용
### 생산자-소비자 문제
- 생산자는 데이터를 생성하여 공유 버퍼에 저장, 소비자는 버퍼에서 데이터 가져가서 사용
- 공유 버퍼가 한정된 크기를 가지기 때문에 버퍼가 다 차면 생산자 대기, 비면  소비자 대기하는 상황 발생
- 생산자는 버퍼가 가득 찬 경우 데이터 추가를 기다리고, 소비자는 버퍼가 빈 경우 데이터 사용을 기다리도록 협력
- 상호 배제를 보장하며 동시 수정 일어나지 않도록 해야 함
- 세마포어를 사용하여 상호 배제 보장
    - 버퍼에 남은 공간을 나타내며 0일 때 생산자 대기
    - 버퍼에 저장된 데이터 나타내며 0일 때 소비자 대기
- 메시지 큐, 버퍼링 시스템에서 사용
### 식사하는 철학자 문제
- 철학자 사이에 포크가 하나씩 놓여있고 양쪽의 포크를 모두 사용해야 할 때 동시에 포크를 들게 되면 교착상태 발생
- 교착상태를 방지하면서 번갈아가면서 식사할 수 있도록 해야 함
- 해결방법
  - 비대칭 방식: 홀수 철학자는 왼쪽, 짝수 철학자는 오른쪽 부터 들도록 설정
  - 최대 개수 조절하여 교착 상태 방지
  - 세마포어나 모니터를 사용하여 방지
  - 한정된 자원을 여러 프러세스가 요청할 때의 상황에서 자원 할당 문제 해결시 사용

## 조건 변수(condition variable)란 무엇인가요?
### 조건 변수
- 여러 스레드가 협력하여 작업을 수행할 때, 특정 조건이 충족될 때까지 쓰레드가 대기하거나 조건이 충족된 스레드를 깨우기 위해 사용하는 동기화 기법
- 일시적인 상태를 위한 동기화 도구
- Wait & Signal(대기와 신호)
  - 대기: 스레드가 어떤 조건이 충족될 때까지 작업을 멈추고 대기할 수 있도록 함
  - 신호: 조건이 충족되었음을 알리기 위해 다른 스레드에게 신호를 보냄
- 동작 원리
  - 스레드가 조건변수를 통해 대기하려면 뮤텍스를 먼저 획득해야 하고, 조건을 기다리는동안 뮤텍스를 해제한 상태에서 대기하게 됨 → 조건이 충족되면 뮤텍스를 다시 획득하여 작업 진행
- 사용 예
  - 생산자 - 소비자 문제
  - 쓰레드 풀

## 메모리 계층 구조에 대해서 설명해주세요.
### 메모리 계층 구조
- 메모리 관련 3가지 주요 특성인 용량, 접근 속도, 비용 간의 절충 관계를 파악하여 나타낸 구조
- 위로 갈수록 속도가 빠르고, 아래로 갈수록 용량이 커짐
- 구성 요소
  - 레지스터: CPU 내부의 작은 메모리, 휘발성이며 속도가 가장 빠르고 용량이 가장 작음, 가장 자주 접근하는 데이터 저장
  - 캐시: L1, L2, L3 캐시로 나뉘며 L1이 가장 빠르고 L3가 용량이 가장 큼, CPU 연산 속도와 메모리 연산 속도 차이를 줄이기 위해 자주 사용하는 데이터와 명령어 저장하여 빠르게 접근
  - 주기억장치(메모리), RAM: 휘발성이고 캐시보다 큰 용량을 제공하지만 속도는 상대적으로 느림
  - 보조기억장치(디스크), HDD, SDD: 비휘발성이고, 시스템에 영구적인 데이터 저장하는 공간으로 사용, 속도 느림
- 성능과 비용 간의 균형을 맞추기 위해 설계 (효율적인 메모리 활용과 성능 최적화를 위해)

## 메모리에서 스택, 힙, 코드, 데이터 영역에 대해서 설명해주세요.
- 코드 영역
  - 실행할 프로그램의 코드가 저장되는 영역(텍스트 영역)
  - 보통 읽기 전용으로 설정되고 메모리 보호 기법을 통해 무결성 보장
- 데이터 영역
  - 프로그램의 전역변수와 정적 변수 저장되는 영역
  - 프로그램 시작시 할당되고 종료되면 소멸
  - 초기화된 영역과 초기화 되지 않은 영역으로 나뉨
- 힙 영역
  - 프로그래머가 동적으로 직접 공간을 할당, 해제 하는 영역
  - FIFO 방식 사용(메모리의 낮은 주소에서 높은 주소 순으로 할당)
  - 명시적으로 해제하지 않으면 프로그램 종료 때까지 유지 - 메모리 누수 문제 발생 가능
- 스택 영역
  - 프로그램이 자동으로 사용하는 임시 메모리 영역
  - 함수 호출시 지역 변수, 매개 변수가 저장되는 영역이고 함수 호출이 완료되면 소멸
  - LIFO 방식 사용 (메모리 높은 주소부터 낮은 주소로 할당되기 때문)
  - 함수가 호출될 때마다 스택에 프레임 추가되고 호출 종료시 프레임 제거

## 가상 메모리(Virtual Memory)란 무엇인가요?
### 가상 메모리
- CPU가 가상주소로 메모리에 접근을 시도하면 MMU(Memory Menagement Unit)가 가상주소를 물리 주소로 변환하여 실제 RAM으로 매핑해주는 기법
- CPU는 RAM에 필요한 모든 페이지가 올라와 있다고 생각하지만 RAM은 제한적이기 때문에 페이지가 RAM에 있을 수 도 있고 DISK에 있을 수 도 있음 → 필요한 페이지를 적절히 RAM에 올려 사용
- 장점
  - 메모리 효율성 향상
  - 프로세스 격리: 각 프로세스는 독립적인 주소 공간을 가지기 때문에 한 프로세스가 다른 프로세스의 메모리에 접근 불가
  - 동적 메모리 할당: 필요한 부분만 메모리에 할당하여 자원 관리
- 단점
  - 페이지 부재로 인한 성능 저하: 필요한 페이지가 없을 때마다 불러와야 하기 때문에 자주 발생하면 성능에 영향을 줌
  - 복잡성 증가

## 페이지 테이블(Page Table)이란 무엇이며, 그 역할을 설명해보세요.
### 페이징
- 논리 주소의 메모리를 고정된 크기의 페이지로 나누어 관리하는 방법
- 페이지는 모두 같은 크기를 가짐
- 프레임: 물리 주소 공간을 페이지와 같은 사이즈로 나눈 것
- 페이지 테이블을 이용해 논리 주소에서 프레임을 가리키는 물리주소로 매핑
### 페이지 테이블
- 가상 메모리 시스템에서 가상 주소를 물리 주소로 변환하기 위해 매핑 정보를 저장하는 데이터 구조
- 운영체제는 각 프로세스마다 페이지 테이블을 유지하며 가상 주소 공간을 물리 주소 공간과 연결함 - 메모리 관리장치(MMU)에 의해 수행

## 페이지 교체 알고리즘(FIFO, LRU, Optimal 등)의 차이점을 설명하세요.
### 페이지 교체 알고리즘
- 가상 메모리에서 페이지 부재가 발생했을 때 메모리에 있는 기존 페이지 중 어느 것을 제거할지 결정하는 방식
- 제한적인 메모리 공간이나 필요성이 적은 페이지를 효율적으로 교체하여 성능 최적화
1. FIFO 알고리즘
  - 메모리에 페이지가 들어온 순서를 큐에 기록하고 오래된 페이지 제거
  - 구현이 간단하고 페이지 교체 대상 결정이 빠르기 때문에 단순한 메모리 관리에 적합
2. LRU 알고리즘
  - 각 페이지가 마지막에 사용된 시간을 기록하여 가장 오랫동안 사용되지 않은 페이지 제거
  - 실제 프로그램의 메모리 접근 패턴과 비슷한 성능을 내기 때문에 성능 우수
  - 페이지 사용 시간을 계속 기록해야 하기 때문에 구현 복잡하고 오버헤드가 큼 → 자주 사용된 페이지가 메모리에 오래 남도록 유지할 때 적합
3. Optimal 알고리즘
  - 앞으로의 메모리 참조를 미리 알고 있다 가정하고, 앞으로 가장 오랫동안 사용되지 않을 페이지 제거
  - 이론적으로 최적의 알고리즘
  - 현실적으로 구현이 불가능
4. LFU 알고리즘
  - 페이지의 참조 횟수를 기록해서 가장 적게 사용된 페이지를 제거
  - 자주 사용되는 페이지를 메모리에 유지할 수 있어 효율적
  - 참조 횟수 기록으로 인한 오버헤드가 크고 최근에 사용된 페이지가 낮은 참조 횟수를 가지는 경우 성능 저하 발생

## 페이지 폴트(Page Fault)란 무엇이며, 어떻게 처리되나요?
### 페이지 폴트
- 프로그램이 접근하려는 페이지가 물리 메모리에 존재하지 않을 때 발생하는 이벤트
- 프로세스가 참조하려는 가상 페이지가 현재 메모리에 로드되어 있지 않은 상태
- 발생 원인
  - 페이지가 메모리에 없음
  - 페이지 교체: 페이지가 디스크로 스왑아웃된 후 다시 필요해진 경우
  - 동적으로 메모리 공간에 접근하려고 할 때 아직 페이지가 로드되지 않은 경우
- 처리 과정
  1. 프로세스가 접근하려는 페이지가 유효하지 않아 페이지 폴트 발생
  2. CPU는 페이지 폴트 예외를 발생시키고 운영체제의 페이지 폴트 처리 루틴으로 제어가 넘어감
  3. 운영체제는 페이지 테이블을 검사해서 요청된 페이지가 메모리에 없는지 확인, 페이지가 있는 디스크 위치 찾음
  4. 메모리에 여유 공간이 있으면 페이지 로드, 여유 공간이 부족하면 페이지 교체 알고리즘 사용하여 교체
  5. 페이지 교체를 하게되면 교체하는 페이지를 디스크에 저장하고 요청 페이지를 메모리로 로드
  6. 페이지가 메모리에 로드되면 페이지 테이블 업데이트하고 해당 페이지의 상태를 유효한 것으로 변경, 물리 메모리 주소 기록
  7. 페이지 폴트가 발생했던 명령어 재실행 → 프로세스 정상 수행
 
## TLB(Translation Lookaside Buffer)의 역할은 무엇인가요?
### TLB
- 주소 변환 속도를 높이기 위해 최근 사용된 가상 주소와 물리 주소의 매핑 정보를 캐싱하는 고속 메모리
- CPU와 메모리 사이에 위치하여 페이지 테이블 접근 횟수를 줄이고 메모리 접근 성능 향상
- 메모리 접근 전 TLB에서 매핑 주소 확인 → 있으면 TLB Hit 발생하여 페이지 테이블 참조하지 않고 물리 주소로 변환 가능
- 매핑 정보가 없으면 TLB Miss 발생하여 페이지 테이블 조회해서 물리 주소 찾고 TLB에 캐싱
- 프로세스가 전환될 때마다 TLB Flush 를 수행하여 기존 캐시 정보 초기화 → 프로세스는 자신만의 주소공간에 접근

## 스와핑(Swapping)이란 무엇이며, 언제 발생하나요?
### 스와핑
- 운영체제가 메모리 공간 확보를 위해 전체 프로세스를 디스크로 내리고 다른 프로세스를 메모리에 올리는 과정
- 메모리 자원이 부족하거나 특정 프로세스를 당장 사용할 필요가 없을 때 해당 프로세스의 메모리를 스왑영역에 저장
- 발생 조건
  - 메모리 부족
  - 오래 사용되지 않은 프로세스 존재
  - 프로세스 실행을 위한 충분한 메모리 확보
- 과정
  - 스왑아웃: 스와핑 대상이 되는 프로세스의 메모리 공간을 스왑영역으로 이동
  - 스왑인: 스왑아웃된 프로세스가 다시 실행되거나 필요해지면 해당 프로세스를 불러옴
- 메모리를 확보하여 멀티태스킹 가능, 메모리 효율적 사용 가능
- 메모리에 접근하는 것보다 속도가 느림 (디스크에 위치)
- 스와핑이 빈번하게 발생하면 시스템 성능이 크게 저하됨(스레싱)
> 스레싱: 스와핑이 과도하게 발생하여 CPU가 프로세스를 실행하기 보다는 페이지 교체에만 자원을 소비하는 상황

## 세그멘테이션(Segmentation)과 페이징(Paging)의 차이점은 무엇인가요?
### 세그멘테이션
- 프로그램을 논리적 단위인 세그먼트로 나누어 관리하는 기법
- 세그먼트는 프로그램의 코드, 데이터, 스택 등을 나타냄
### 페이징
- 프로그램의 메모리 공간을 고정 크기의 페이지로 나누고 물리적 메모리도 고정 크기의 프레임으로 나누어 관리하는 기법
- 논리적 주소는 페이지 번호와 오프셋으로 구분
  - 페이지 번호: 페이지 테이블을 통해 해당 페이지가 물리적 메모리에서 어디에 위치하는지 알 수 있음
  - 오프셋: 페이지 내에서의 상대적인 위치

## 메모리 단편화(Fragmentation)란 무엇이며, 내부 단편화와 외부 단편화의 차이점은 무엇인가요?
### 메모리 단편화
프로그램 실행 중에 메모리 할당과 해제를 반복하면서 사용 가능한 연속된 메모리 공간이 부족해지거나 남은 메모리가 비효율적으로 분산되는 문제
### 내부 단편화
- 메모리를 할당받은 블록 내부에 남는 불필요한 공간(할당할 때 요청보다 더 큰 크기의 블록이 할당되어 일부가 사용되지 않아 낭비되는 경우 발생)
- 해결방법
  - 동적으로 가변 크기의 메모리 블록을 할당하여 낭비되는 공간을 줄임
  - 필요한 크기만큼만 메모리를 할당하고 메모리 할당과 해제를 관리하는 방식
### 외부 단편화
- 메모리 내에 작은 빈 공간들이 여러 곳에 흩어져 있어 충분한 연속된 메모리 공간을 확보할 수 없게 되는 현상
- 메모리 할당과 해제를 반복하면서 작은 빈 공간들이 메모리의 여러 부분에 분산되기 때문에 연속적인 큰 공간 확보 불가
- 해결방법
  - 빈 공간을 모아서 연속된 큰 공간을 만들어 외부 단편화 해소(시간이 많이 걸리고 성능에 영향을 줄 수 있음)
  - 고정 크기의 슬롯을 사용하여 외부 단편화 줄임
 
## 페이지 크기(Page Size)가 메모리 관리에 미치는 영향은 무엇인가요?
### 페이지 크기
- 페이징 메모리 관리 기법에서 메모리 효율성, 성능, 단편화에 영향을 줌
- 시스템의 논리적 주소 공간을 고정 크기의 페이지로 나누고 이를 물리적 주소 공간의 고정 크기 프레임에 매핑할 때 사용
### 메모리 관리에 미치는 영향
1. 페이지 크기가 크면 사용되지 않는 공간이 많아져 내부 단편화 증가 ↔ 작으면 내부 단편화는 줄지만 너무 작으면 페이지 테이블의 크기가 비효율적으로 커짐
2. 페이지 크기가 크면 페이지 테이블의 크기는 작아지지만 저장할 수 있는 정보가 줄어들어 여러 페이지를 관리해야 하는 경우 성능에 문제 생김 ↔ 작으면 전체 메모리를 페이지로 나누기 위해 필요한 페이지 수가 늘어나기 때문에 페이지 테이블이 커지고 관리하는 비용 증가
3. 페이지 크기가 크면 TLB 캐시 적중률이 높아지지만 메모리 낭비 발생 가능 ↔ 작으면 TLB를 더 자주 비우고 채워야 하기 때문에 TLB Miss 발생 증가
4. 페이지 크기가 너무 크면 캐시의 효율성이 떨어지고 프로그램이 자주 참조하는 데이터가 한 페이지 내에 모두 포함되지 않아 잦은 페이지 교체 발생 ↔ 작으면 페이지 테이블을 관리하는 오버헤드가 커지고 TLB와 같은 캐시가 효율적으로 작동하지 않아 성능 저하 발생

## SSD와 HDD의 차이점은 무엇인가요?
### SDD
- 비휘발성 플래시 메모리를 사용하여 데이터 저장(NAND 플래시 메모리)
- 데이터 읽고 쓰는 속도가 훨씬 빠름
- 기계적 부품이 없어서 충격에 강하고 소음과 진동이 없음
- 비교적 용량이 적은 편
- 전력 소비가 적지만 가격이 비싼 편
## HDD
- 회전하는 디스크와 읽기/쓰기 헤드가 있어 데이터를 물리적으로 읽고 씀
- 상대적으로 데이터 읽고 쓰는 속도가 느림
- 기계적 부품이 있어서 충격에 취약하고 소음과 진동 있음
- 대용량
- 전력 소비가 많고 상대적으로 저렴한 편

## 캐시(Caching)는 운영 체제에서 어떤 역할을 하며, 성능 향상에 어떻게 기여하는가요?
### 캐시
- 컴퓨터 시스템에서 자주 사용되는 데이터나 연산 결과를 빠르게 접근할 수 있도록 임시로 저장해두는 기술
- 저장소와 처리 속도 차이를 극복하기 위해 설계된 메모리 계층
- CPU 캐시: CPU 내부에 위치하며 CPU가 자주 사용하는 데이터를 메인 메모리보다 빠른 속도로 접근할 수 있도록 도와줌
- 디스크 캐시: HDD나 SDD에서 데이터를 읽을 때 디스크의 데이터를 캐시에 저장
- 페이지 캐시: 메모리에 페이지 단위로 파일 데이터 저장
- 웹 캐시: 웹 서버나 클라이언트에서 자주 요청되는 데이터를 저장하여 대역폭 절약하고 응답시간 단축
- 메모리 캐시: 애플리케이션에서 자주 사용하는 데이터를 RAM에 저장하는 방식
- 성능 향상 기여 방법
  - 데이터 접근 시간 단축
  - 불필요한 데이터 접근 방지, 불필요한 중복 작업 방지
  - 병목 현상 해소: 디스크는 메모리보다 느리기 때문에 디스크에 접근할 때 발생하는 지연이 성능 저하 발생 가능
  - 반복적인 연산 최적화

## 캐시 히트(Cache Hit)와 캐시 미스(Cache Miss)란 무엇이며, 캐시 히트율(Cache Hit Rate)은 어떻게 측정되는가요?
### 캐시 히트(Cache Hit)
CPU나 애플리케이션이 요청한 데이터가 캐시에 이미 저장되어 있어 캐시에서 직접 데이터를 가져올 수 있는 경우
### 캐시 미스(Cache Miss)
- 요청된 데이터가 캐시 내에 없는 경우
- 메인 메모리, 디스크 등 저장소에서 데이터 검색 → 데이터 접근 속도 저하
### 캐시 히트율(Cache Hit Rate)
- 전체 요청 중에서 캐시 히트가 발생한 비율
- 히트율이 높을수록 캐시의 효율성이 좋음 → 데이터를 빠르게 처리하여 시스템 성능 향상

- 캐시 히트율 높이는 방법
  - 적절한 캐시 크기 설정
  - 캐시 교체 알고리즘 최적화
  - Prefetching: 예측 가능한 데이터 패턴에 대해 미리 데이터를 캐시로 가져오는 방법
  - 데이터 접근 패턴 분석

## 캐시의 공간적 지역성과 시간적 지역성에 대해서 설명해주세요
### 공간적 지역성
- 현재 접근한 데이터와 물리적으로 인접한 데이터들이 가까운 시점에 함께 접근될 가능성이 높다는 개념(연속된 데이터를 요청할 가능성이 높다는 것)
- 예시: 긴 웹 페이지를 스크롤한다면 페이지 위쪽을 본 후 아래쪽을 볼 가능성이 높기 때문에 페이지의 특정 부분과 인접한 부분을 함께 요청
### 시간적 지역성
- 최근에 접근한 데이터는 가까운 미래에도 다시 접근할 가능성이 높다는 개념(어떤 데이터에 접근한 후 다시 그 데이터에 접근할 확률이 높은 경우)
- 예시: 로그인 후 대시보드 페이지를 본 다음 다시 대시보드를 열 때 데이터가 캐시되어 있으면 바로 제공 가능

## 버퍼 캐시(Buffer Cache)와 페이지 캐시(Page Cache)의 차이점은 무엇이며, 파일 시스템에서 각각의 역할은 무엇인가요?
### 버퍼 캐시
- 블록 단위로 데이터를 저장하는 캐시
- 디스크 블록 단위로 데이터를 처리하고 디스크의 블록 장치 I/O를 줄이기 위해 데이터를 캐싱
- 디스크 상의 모든 종류의 데이터를 캐시
- 파일 시스템에서 디스크 블록 위치를 추적하는 메타데이터를 캐시하여 빠르게 접근
- 파일에 데이터를 쓸 때 먼저 버퍼 캐시에 데이터를 기록하고 나중에 디스크에 반영하는 지연쓰기 방식을 사용하여 성능 최적화
### 페이지 캐시
- 파일 시스템의 데이터 페이지를 캐시하는 메모리 영역
- 파일을 페이지 단위로 처리하고 파일을 읽을 때 파일의 내용을 페이지 단위로 메모리에 로딩
- 주로 파일 데이터와 관련된 내용만 캐시(메타데이터 x)
- 메모리 상에서 파일을 읽을 때 파일의 내용을 가상 주소 공간에 매핑하여 빠르게 데이터에 접근
- 페이지 캐시가 활성화되면 파일 데이터를 메모리에 캐시하여 디스크에서 파일을 다시 읽는 비용 최소화

## 파일 시스템이란 무엇인가요?
### 파일 시스템
- 파일과 디렉토리를 생성하고 관리, 데이터 효율적이고 안전하게 저장하고 접근할 수 있도록 해줌
- 데이터를 하드 드라이브, SSD와 같은 저장 장치에 저장하고 데이터를 읽고 쓰는 방법 관리
- 역할
  - 파일 저장 및 관리
  - 파일 이름 관리
  - 파일 접근 제어
  - 디렉토리 구조 제공
  - 저장 장치의 공간 관리
- 구성 요소
  - 파일: 데이터를 저장하는 기본 단위, 이름, 데이터 내용, 파일 속성 가짐
  - 디렉토리: 파일 시스템 내에서 파일들을 조직, 트리 구조로 되어 있어서 계층적 파일 시스템 형성
  - 메타데이터: 파일 크기, 생성 시간, 수정 시간 등 파일에 대한 속성 정보
  - 블록: 저장 장치의 데이터를 블록 단위로 관리, 일정 크기로 정의되며 데이터를 저장할 때 블록으로 나누어 저장
- 동작
  - 저장: 사용자가 새로운 파일 생성하면 파일 시스템은 파일을 디스크 블록에 저장하고 저장위치는 파일 할당 테이블 또는 디스크 매핑 구조에 기록
  - 조회: 사용자가 파일을 읽으려 할 때 파일 시스템은 파일 이름을 사용해 해당 파일의 메타데이터를 조회하고 파일이 저장된 디스크 블록을 찾아 데이터 읽음
  - 수정: 파일을 수정하면 파일 시스템은 새로운 데이터를 블록에 덮어쓰기하거나 빈블록에 새 데이터를 저장하고 기존 파일의 블록을 업데이트
  - 삭제: 파일시스템은 해당 파일의 블록을 해제하고 파일의 메타데이터와 위치 정보를 삭제

## 파일 시스템에서 inode의 역할은 무엇인가요?
- 파일 시스템에서 파일에 대한 메타 데이터 저장
- 메타 데이터에 파일의 물리적 위치, 구체적인 내용을 제외한 파일의 속성 정보를 포함
- 파일시스템에서 각 파일마다 고유한 inode 번호 할당 → 해당 파일에 대한 중요한 정보를 찾기 위한 인덱스 역할
- 파일 이름과 분리되어 관리되며 디렉토리는 이름과 inode 번호를 연결하여 파일을 찾을 수 있음

## 파일 디스크립터(File Descriptor)는 무엇인가요?
- 운영체제에서 프로세스와 파일 시스템 간의 인터페이스 역할
- 프로세스는 파일 시스템에서 파일을 열고 읽고 쓰는 등의 작업 수행, 파일을 열 때 생성되고 닫을 때 해제됨
- 운영체제의 커널에서 관리하고 파일, 소켓, 파이프, 장치 파일 등 입출력 자원을 관리할 때 사용
- 동작 원리
  - 파일 열기: `open()` 시스템 호출하여 파일을 열면 운영체제는 해당 파일에 대한 파일 디스크립터 할당,
  - 파일 디스크립터 테이블: 각 프로세스는 자신만의 파일 디스크립터 테이블을 가지고 있음 → 테이블에는 파일 디스크립터 번호와 해당 파일에 대한 파일 정보 연결
    - 파일 읽기, 쓰기, 파일 속성 변경 등의 작업 수행, `read()`, `write()` 시스템 호출
    - 프로세스는 파일 디스크립터를 통해 파일에 접근하고 이 정보를 기반으로 입출력 수행
  - 파일 닫기: 파일 작업을 완료하면 `close()` 시스템 호출하여 파일 닫고 파일 디스크립터 반환 → 파일 디스크립터 테이블에서 항목 제거하고 자원 해제하는 역할

## 파일 시스템에서 링크(link)란 무엇이며, 하드 링크와 심볼릭 링크의 차이점에 대해서 설명해주세요
### 링크
  - 파일 시스템 내에서 파일을 참조하는 방법
  - 하나의 파일에 여러 이름을 붙여서 파일을 참조할 수 있도록 하는 메커니즘
  - 동일한 파일을 여러 위치에서 접근 할 수 있게 되어 파일의 중복을 피하고 효율적으로 관리 가능
### 하드 링크
  - 파일의 실제 데이터를 직접 참조하는 파일 엔트리
  - 파일 시스템에서 파일의 이름을 inode와 연결하는 방식
  - 원본 파일의 실제 데이터를 참조하는 새로운 파일 엔트리를 만들기 때문에 원본 파일과 동일한 inode 번호 가짐
  - 파일의 데이터 블록을 공유하므로 실제 파일의 데이터는 하나만 존재
  - 하드 링크를 사용한 파일은 모든 하드 링크가 삭제되기 전까지 데이터 유지
  - 같은 파일 시스템 내에서만 생성 가능
### 심볼릭 링크
  - 파일이나 디렉토리의 경로를 참조하는 별도의 파일(간접적인 참조)
  - 파일 시스템 내의 다른 파일을 가리키는 경로를 포함하는 텍스트 파일
  - 다른 파일 시스템에 있는 파일 참조 가능(링크 생성 가능)
  - 원본 파일이나 디렉토리가 삭제되면 링크가 깨질 수 있음 → 대상 파일 없음 등의 오류 발생
  - 원본 파일과 다른 inode 번호를 가짐, 링크 파일은 경로 정보를 담고 있는 파일이고 원본 파일의 데이터를 직접 포함하지 않음

## 저널링 파일 시스템(Journaling File System)이란 무엇인가요?
### 저널링 파일 시스템
- 데이터 무결성과 안정성을 보장하기 위해 파일 시스템의 메타 데이터 및 파일 데이터를 로그 형태로 기록하는 기술
- 파일 시스템의 변경 사항을 저널이라는 특수한 로그 파일에 기록 → 파일 시스템에서 발생하는 트랜잭션을 순차적으로 기록하고 실제 파일 시스템에 적용하기 전에 로그 기록
- 동작
  - 트랜잭션 기록: 파일 시스템에서 데이터나 메타데이터에 변경 사항이 발생할 때 변경 작업은 저널에 기록되고 저널에 기록된 내용은 시스템이 안정적일 때 파일 시스템에 반영
  - 커밋(commit): 저널에 기록된 변경 사항이 모두 완료되면 실제 파일 시스템에 변경 사항 적용되는 작업
  - 복구(recovery): 시스템이 비정상적으로 종료될 경우 파일 시스템을 저널에 기록된 변경사항을 사용하여 복구 작업 수행 → 트랜잭션 일관성을 유지하고 데이터 손실 최소화 가능
 
## 디스크 스케줄링이 필요한 이유와 다양한 디스크 스케줄링 알고리즘(FCFS, SSTF, SCAN 등)에 대해서 설명해주세요.
### 디스크 스케줄링
- 디스크 헤드가 요청된 데이터를 읽거나 쓰기 위한 최적의 순서를 결정하는 방법
- 디스크 헤드의 이동을 최적화하여 이런 탐색 시간과 회전 지연을 최소화하고 디스크 I/O 작업을 효율적으로 처리하기 위해 사용 → 디스크 응답 시간을 줄이고 시스템 성능 향상
> 디스크는 컴퓨터에서 데이터를 영구적으로 저장하는 장치로 물리적으로 여러개의 플레터와 헤드가 데이터를 읽거나 쓰기 위해 움직여야 함 → 디스크 헤드가 데이터를 찾기 위해 이동하는 시간이나 디스크의 회전으로 인해 데이터를 찾는 데 걸리는 시간이 성능에 영항을 줌

### 디스크 스케줄링 알고리즘
- FCFS: 요청 순서대로 처리, 구현은 간단하지만 비효율적으로 동작하며 탐색 시간이 길어짐
- SSTF: 현재 헤드 위치에서 가장 가까운 트랙에 대한 요청 먼저 처리, 효율적이지만 starvation 발생 가능
- SCAN: 헤드가 한방향으로 이동해서 끝까지 가고 다시 반대 방향으로 이동하며 요청 처리, 효율적이고 starvation 방지 가능하지만 회전 지연 발생 가능
- LOOK: SCAN처럼 끝까지 가지 않고 최대 요청이 있는 위치까지만 이동 후 다시 반대 방향으로 이동, SCAN 보다 효율적이지만 요청이 많은 트랙이 끝에 있으면 비효율적

## 운영체제에서 마운트란 무엇이며, 운영체제에서 어떻게 동작되는가
### 마운트
- 운영체제에서 파일 시스템을 활성화하고 이를 사용 가능한 형태로 연결하는 과정
- 외부 저장장치의 파일 시스템을 운영체제의 기존 디렉토리 트리에 연결하여 사용자가 그 파일 시스템에 접근할 수 있도록 만드는 작업
- 파일 시스템 트리는 하나의 계층적 디렉토리 구조로 구성되며 새로운 파일 시스템이 이 구조의 특정 지점에 연결됨
### 동작 원리
- 디스크 장치 탐지: 연결된 외부 저장 장치 탐지
- 파일 시스템 인식: 해당 저장장치의 파일 시스템 형식 인식하고 지원하기 위해 준비
- 마운트 지점 지정: 파일 시스템을 연결할 디렉토리 선택 → 마운트 지점
- 파일 시스템 연결: 저장 장치의 파일 시스템을 마운트 지점에 연결하여 해당 경로를 통해 파일에 접근하도록 함

## 동기와 비동기 그리고 병렬성에 대한 차이점에 대해서 설명해주세요.
### 동기와 비동기
- 동기
  - 작업을 순차적으로 처리하고 각 작업은 이전 작업이 끝날때까지 대기
  - 작업이 완료될때까지 블로킹 상태 → 한 작업 끝난 후 결과를 받아 다음 작업 수행
- 비동기
  - 작업은 차례대로가 아닌 병렬적, 독립적으로 수행되고 이전 작업이 끝나지 않아도 다른 작업 수행 가능
  - 결과를 기다리지 않고 다른 작업 동시에 수행 → 완료 후 다른 작업에 영향을 주지 않음
  - callback이나 promise 등의 방법으로 처리
### 병렬성
- 여러 작업이 동시에 실행되는 개념
- 여러 개의 프로세서나 코어를 사용하여 다수의 작업을 병렬로 처리
- 멀티코어 프로세서나 분산 시스템을 활용할 때 중요, 멀티 스레딩, 대규모 데이터 처리 시에 사용
- 비동기와 병렬성 차이
  - 비동기는 작업을 동시에 시작할 수 있지만 작업이 실제로 동시에 실행되는 것은 아님 - CPU의 하나의 스레드에서 여러 작업을 효율적으로 다루는 방식
  - 병렬성은 하드웨어적으로 여러 작업을 동시에 실행하는 것

## CPU 개수가 증가할수록 시스템의 성능이 좋아지는 이유에 대해서 설명해주세요.
- 여러 CPU가 존재하면 각 CPU는 동시에 별개의 작업 처리 가능
- 여러 CPU가 있으면 복잡한 작업을 여러 부분으로 나누어 각 CPU에 효율적으로 분배 가능하며 각 CPU는 상대적으로 짧은 시간안에 작업을 끝낼 수 있고 다른 CPU를 기다리지 않기 때문에 처리 속도가 빨라짐
- 멀티스레딩과 멀티태스킹 효과적으로 구현 가능, 한 프로세서가 동시에 여러 스레드를 실행하거나 여러 프로세스를 동시에 처리 가능 → 각 스레드나 프로세스가 독립적으로 실행되므로 응답속도와 반응성이 좋아짐
- 여러개의 CPU가 동시다발적으로 실행되면서 시스템 자원을 더 효율적으로 활용할 수 있기 때문에 시스템의 각 자원이 최대한 활용되면서 성능 최적화 가능

## 멀티코어 CPU 환경에서 자바의 병렬 처리 성능을 최적화하기 위해 코어 수와 스레드 수의 균형을 어떻게 맞춰야 하는가요?

## CPU 바운드(CPU-bound) 작업과 I/O 바운드(I/O-bound) 작업의 특성에 따라 병렬 처리 전략을 어떻게 달리 설정해야 하는가?
### CPU 바운드 작업
- CPU의 처리 능력에 의해 제한되고 주로 복잡한 계산, 알고리즘 실행, 데이터 처리 등의 작업을 수행
- 병렬 처리 전략
  - 스레드 수 = 코어 수: 각 코어가 고유한 작업을 처리하고 컨택스트 스위칭에 의한 오버헤드 최소화 가능
  - 가능한 작은 단위로 작업을 나누어 병렬 처리
  - 스레드 풀을 사용하여 작업 분배하고 스레드를 재사용하는 것이 중요함
### I/O 바운드 작업
- 디스크 I/O, 네트워크 I/O, 파일 읽기/쓰기, 데이터베이스 쿼리 등 외부 시스템과의 상호작용에 의존하는 작업
- 병렬 처리 전략
  - 스레드 수 > 코어 수: CPU를 많이 사용하지 않기 때문에 코어 수를 많이 설정하여 여러 스레드가 동시에 대기 상태에 있을 수 있게 만듬 → 하나의 스레드가 I/O 작업을 기다리는동안 다른 스레드가 CPU를 사용할 수 있게 되기 때문에 CPU 작업을 낭비하지 않고 동시에 여러 작업 처리 가능
  - 스레드 풀을 사용하여 효율적으로 스레드 관리하고 스레드 수 최적화 필요
  - 비동기 방식으로 I/O 작업을 처리하여 블로킹 없이 다른 작업 동시에 수행 가능

## JVM에서 Garbage Collection이 병렬 처리 성능에 미치는 영향은 무엇이며, 이를 최적화하기 위한 JVM 튜닝 방법은?
- JVM에서 Garbage Collection이 병렬 처리 성능에 미치는 영향
  - Stop-the-World: GC가 실행되는동안 모든 애플리케이션 스레드가 멈추므로 GC가 자주 발생하거나 오래 걸리면 병럴 처리 성능에 부정적인 영향을 줌
  - GC 시간: 시간이 길어지면 애플리케이션의 응답성 저하됨
  - 메모리 부족시 GC가 자주 발생되고 성능 저하 발생 가능
- JVM 튜닝 방법
  - GC 알고리즘 선택: 일반적으로 Parallel GC나 G1 GC를 사용
  - 힙 메모리 설정을 통해 GC 빈도와 시간 최적화
  - GC 로그를 통해 성능 모니터링 하고 GC가 자주 발생하거나 시간이 길어지지 않도록 조정
  - 병렬 GC에 사용하는 스레드 수를 설정하여 성능 향상
